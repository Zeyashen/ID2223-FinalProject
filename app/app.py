import gradio as gr
import hopsworks
import joblib
import pandas as pd
import os
import openmeteo_requests
import requests_cache
from retry_requests import retry
from huggingface_hub import InferenceClient

# ==========================================
# ğŸ§  Model 1: é¢„æµ‹ç®¡é“ (è·å–ç¡¬æ•°æ®)
# ==========================================
def get_weather_forecast():
    cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)
    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)
    openmeteo = openmeteo_requests.Client(session = retry_session)

    params = {
        "latitude": 63.399, "longitude": 13.082, # Ã…re
        "daily": ["temperature_2m_max", "precipitation_sum", "wind_speed_10m_max", "snowfall_sum"],
        "timezone": "Europe/Berlin", "forecast_days": 7
    }
    
    responses = openmeteo.weather_api("https://api.open-meteo.com/v1/forecast", params=params)
    response = responses[0]
    daily = response.Daily()
    
    df = pd.DataFrame({
        "date": pd.date_range(
            start = pd.to_datetime(daily.Time(), unit = "s", utc = True),
            end = pd.to_datetime(daily.TimeEnd(), unit = "s", utc = True),
            freq = pd.Timedelta(seconds = daily.Interval()),
            inclusive = "left"
        ),
        "temperature_max": daily.Variables(0).ValuesAsNumpy(),
        "precipitation": daily.Variables(1).ValuesAsNumpy(),
        "wind_gusts": daily.Variables(2).ValuesAsNumpy(),
        "snowfall_sum": daily.Variables(3).ValuesAsNumpy(),
    })
    df['date_str'] = df['date'].dt.strftime('%Y-%m-%d')
    return df

def get_prediction_data():
    print("ğŸ¤– Model 1 working: Connecting to Hopsworks...")
    try:
        # è¿™é‡Œçš„ login ä¼šè‡ªåŠ¨è¯»å– Secrets é‡Œçš„ HOPSWORKS_API_KEY
        project = hopsworks.login(project="zeyashen")
        mr = project.get_model_registry()
        model = mr.get_model(name="ski_depth_model", version=1)
        model_dir = model.download()
        
        model_path = os.path.join(model_dir, "sklearn_ski_model.pkl")
        trained_model = joblib.load(model_path)
        
        df = get_weather_forecast()
        features = df[['temperature_max', 'precipitation', 'wind_gusts', 'snowfall_sum']]
        
        preds = trained_model.predict(features)
        preds = [max(0, p) for p in preds] # ä¿®æ­£è´Ÿæ•°
        
        summary = ""
        for date, snow, temp in zip(df['date_str'], preds, df['temperature_max']):
            summary += f"- {date}: Temp {temp:.1f}Â°C, Predicted Snow Depth {snow:.1f}cm\n"
        
        return summary
    except Exception as e:
        print(f"Error: {e}")
        return "Error fetching data. Please check logs."

print("â³ Initializing: Fetching latest data and model...")
CACHE_FORECAST = get_prediction_data()
print("âœ… Data ready!")

# ==========================================
# ğŸ—£ï¸ Model 2: Hugging Face LLM (åˆ›æ„å¯¹è¯)
# ==========================================
def chatbot_response(message, history):
    token = os.environ.get("HF_TOKEN")
    if not token:
        yield "âš ï¸ Error: HF_TOKEN is missing. Please add it in Settings -> Secrets."
        return

    # # ğŸ”„ ä¿®æ”¹ç‚¹ï¼šæ¢æˆäº†æ›´ç¨³å®šçš„ Microsoft Phi-3.5 æ¨¡å‹
    # client = InferenceClient("microsoft/Phi-3.5-mini-instruct", token=token)

    client = InferenceClient("Qwen/Qwen2.5-7B-Instruct", token=token)
    
    system_prompt = f"""
    You are 'SnowBot', a funny ski instructor in Ã…re, Sweden.
    REAL forecast data:
    {CACHE_FORECAST}
    Rules: Short answer. Use emojis. Be sarcastic if no snow, excited if deep snow.
    """

    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(history)
    messages.append({"role": "user", "content": message})

    try:
        partial_message = ""
        for token in client.chat_completion(messages, max_tokens=500, stream=True):
            if token.choices[0].delta.content:
                partial_message += token.choices[0].delta.content
                yield partial_message
    except Exception as e:
        # è¿™é‡Œä¼šæŠŠè¯¦ç»†é”™è¯¯æ‰“å°åœ¨å¯¹è¯æ¡†é‡Œï¼Œæ–¹ä¾¿è°ƒè¯•
        yield f"LLM Error: {str(e)}"

# ==========================================
# ğŸ¨ Gradio ç•Œé¢
# ==========================================
demo = gr.ChatInterface(
    fn=chatbot_response,
    title="ğŸ¿ Ã…re Ski Forecast Bot",
    description="System 2: XGBoost Prediction + Zephyr-7B LLM.",
    examples=[
        "Is it worth going skiing tomorrow?",
        "How is the snow on the weekend?",
    ],
    # å…³é”®ä¿®å¤ï¼šå…³é—­ç¤ºä¾‹ç¼“å­˜ï¼Œé˜²æ­¢å¯åŠ¨æ—¶å´©æºƒ
    cache_examples=False
)

if __name__ == "__main__":
    demo.launch()