import gradio as gr
import hopsworks
import joblib
import pandas as pd
import os
import openmeteo_requests
import requests_cache
from retry_requests import retry
from huggingface_hub import InferenceClient # <--- æ›¿æ¢äº† OpenAI

# ==========================================
# ğŸ”‘ é…ç½®åŒºåŸŸ
# ==========================================
# ç¡®ä¿ä½ çš„ç¯å¢ƒä¸­é…ç½®äº† HF_TOKEN
# æˆ–è€…ä¸´æ—¶å¡«åœ¨è¿™é‡Œ (æäº¤åˆ° GitHub å‰è®°å¾—åˆ æ‰ï¼)
if "HF_TOKEN" not in os.environ:
    os.environ["HF_TOKEN"] = "bcjidsfbhewqilfewuyi768321bfhewqvbhkl290427bdkscdskvs"

# ==========================================
# ğŸ§  Model 1: é¢„æµ‹ç®¡é“ (è·å–ç¡¬æ•°æ® - ä¸å˜)
# ==========================================
def get_weather_forecast():
    cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)
    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)
    openmeteo = openmeteo_requests.Client(session = retry_session)

    params = {
        "latitude": 63.399, "longitude": 13.082, # Ã…re
        "daily": ["temperature_2m_max", "precipitation_sum", "wind_speed_10m_max", "snowfall_sum"],
        "timezone": "Europe/Berlin", "forecast_days": 7
    }
    
    responses = openmeteo.weather_api("https://api.open-meteo.com/v1/forecast", params=params)
    response = responses[0]
    daily = response.Daily()
    
    df = pd.DataFrame({
        "date": pd.date_range(
            start = pd.to_datetime(daily.Time(), unit = "s", utc = True),
            end = pd.to_datetime(daily.TimeEnd(), unit = "s", utc = True),
            freq = pd.Timedelta(seconds = daily.Interval()),
            inclusive = "left"
        ),
        "temperature_max": daily.Variables(0).ValuesAsNumpy(),
        "precipitation": daily.Variables(1).ValuesAsNumpy(),
        "wind_gusts": daily.Variables(2).ValuesAsNumpy(),
        "snowfall_sum": daily.Variables(3).ValuesAsNumpy(),
    })
    df['date_str'] = df['date'].dt.strftime('%Y-%m-%d')
    return df

def get_prediction_data():
    """
    è¿è¡Œæ¨¡å‹é¢„æµ‹ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«æœªæ¥7å¤©æ•°æ®çš„å­—ç¬¦ä¸²æ‘˜è¦
    """
    print("ğŸ¤– Model 1 working: Connecting to Hopsworks...")
    try:
        project = hopsworks.login()
        mr = project.get_model_registry()
        model = mr.get_model(name="ski_depth_model", version=1)
        model_dir = model.download()
        
        model_path = os.path.join(model_dir, "sklearn_ski_model.pkl")
        trained_model = joblib.load(model_path)
        
        df = get_weather_forecast()
        features = df[['temperature_max', 'precipitation', 'wind_gusts', 'snowfall_sum']]
        
        preds = trained_model.predict(features)
        preds = [max(0, p) for p in preds] # ä¿®æ­£è´Ÿæ•°
        
        # æ„å»ºæ•°æ®æ‘˜è¦
        summary = ""
        for date, snow, temp in zip(df['date_str'], preds, df['temperature_max']):
            summary += f"- {date}: Temp {temp:.1f}Â°C, Predicted Snow Depth {snow:.1f}cm\n"
        
        return summary
    except Exception as e:
        return f"Error fetching predictions: {str(e)}"

# å¯åŠ¨æ—¶å…ˆè·å–ä¸€æ¬¡æ•°æ®ï¼Œå­˜å…¥ç¼“å­˜
print("â³ Initializing: Fetching latest data and model...")
CACHE_FORECAST = get_prediction_data()
print("âœ… Data ready!")

# ==========================================
# ğŸ—£ï¸ Model 2: Hugging Face LLM (åˆ›æ„å¯¹è¯)
# ==========================================
def chatbot_response(message, history):
    """
    ä½¿ç”¨ Hugging Face Inference API è¿›è¡Œå¯¹è¯
    """
    
    # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
    # æˆ‘ä»¬ä½¿ç”¨ Zephyr-7b-betaï¼Œå®ƒæ˜¯ Lab 2 ä¸­å¸¸ç”¨çš„ Mistral çš„ä¼˜åŒ–ç‰ˆï¼Œå¯¹è¯èƒ½åŠ›å¾ˆå¼º
    # ä½ ä¹Ÿå¯ä»¥æ¢æˆ "mistralai/Mistral-7B-Instruct-v0.3"
    client = InferenceClient(
        "HuggingFaceH4/zephyr-7b-beta", 
        token=os.environ["HF_TOKEN"]
    )
    
    # 2. å®šä¹‰è§’è‰² (System Prompt)
    system_prompt = f"""
    You are 'SnowBot', a funny and slightly sarcastic ski instructor in Ã…re, Sweden.
    
    Here is the REAL forecast (from our ML model) for the next 7 days:
    {CACHE_FORECAST}
    
    Rules:
    1. Answer based on the data above.
    2. If snow < 10cm, be pessimistic and sarcastic.
    3. If snow > 30cm, be super excited!
    4. Keep answers short (under 3 sentences).
    5. Use emojis!
    """

    # 3. æ„å»º Prompt (æ ¼å¼åŒ–ä¸º Chat ç»“æ„)
    messages = []
    messages.append({"role": "system", "content": system_prompt})
    
    # æ·»åŠ å†å²è®°å½•
    for user_msg, bot_msg in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": bot_msg})
    
    messages.append({"role": "user", "content": message})

    # 4. ç”Ÿæˆå›å¤
    # stream=True è®©å›å¤åƒæ‰“å­—æœºä¸€æ ·å‡ºæ¥ï¼Œä½“éªŒæ›´å¥½
    partial_message = ""
    for token in client.chat_completion(messages, max_tokens=500, stream=True):
        if token.choices[0].delta.content:
            partial_message += token.choices[0].delta.content
            yield partial_message

# ==========================================
# ğŸ¨ Gradio ç•Œé¢
# ==========================================
demo = gr.ChatInterface(
    fn=chatbot_response,
    title="ğŸ¿ Ã…re Ski Forecast Bot (System 2 Design)",
    description="I use a Prediction Model (XGBoost/Sklearn) for facts, and an LLM (Zephyr-7B) for personality.",
    examples=[
        "Is it worth going skiing tomorrow?",
        "How is the snow on the weekend?",
        "Should I bring my rock skis?",
    ],
    theme="soft"
)

if __name__ == "__main__":
    demo.launch()